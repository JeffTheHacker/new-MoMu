# MoleculeCaption
 This repository contains the code of the downstream task (molecule caption) in the paper "Natural Language-informed Understanding of Molecule Graphs‚Äù

# Acknowledgment

We adapted the code of the PyTorch implementation of [MolT5](https://github.com/blender-nlp/MolT5/tree/main/evaluation). Thanks to the original authors for their work!

# Installation

The requirements for the evaluation code conda environment are in environment_eval.yml. An environment can be created using the following commands:

```
conda env create -n MolTextTranslationEval -f environment_eval.yml python=3.9
conda activate MolTextTranslationEval
python -m spacy download en_core_web_sm
pip install git+https://github.com/samoturk/mol2vec
```


# MolT5 & SciBert Checkpoints

Before jointly training the MolT5 model and GIN model in the molecule caption task, you should download three MolT5 checkpoints with different sizes on [huggingface](https://huggingface.co/laituan245). After downloading, put them into three folders `molt5-small/`, `molt5-base/`, `molt5-large/`, respectively. The final directory should look like this:

```
--molt5-small or molt5-base or molt5-large
  --config.json
  --pytorch_model.bin
  --special_tokens_map.json
  --spiece.model
  --tokenizer.json
  --tokenizer_config.json
```

Before evaluation, you should download the SciBert checkpoint on [huggingface](https://huggingface.co/allenai/scibert_scivocab_uncased). After downloading, put them into the `scibert/` folder. The final directory should look like this:

```
--scibert
  --config.json
  --flax_model.msgpack
  --pytorch_model.bin
  --vocab.txt
```

# Our Pretrained models

To better utilize the structural information of the input molecule for translation, we append the graph feature of the molecular graph to the inputs of the MolT5 encoder through a feature mapping module, which is implemented by a multi-layer perceptron. The graph features are extracted by the graph encoder GIN in MoMu-K or MoMu-S. You can download them on [the Baidu Netdisk](https://pan.baidu.com/s/1jvMP_ysQGTMd_2sTLUD45A), the password is **1234**. 

MoMu-K checkpoint:

```
checkpoints/littlegin=graphclinit_bert=kvplm_epoch=299-step=18300.ckpt
```

MoMu-S checkpoint:

```
checkpoints/littlegin=graphclinit_bert=scibert_epoch=299-step=18300.ckpt
```

After downloading, you should put these two checkpoints into the `checkpoints/` folder.

# Finetune & Generate captions

Finetune on MoMu-S:
```
python main_transformer_smiles2caption.py --mode train --model_size base
```

or finetune on MoMu-K:
```
python main_transformer_smiles2caption.py --mode train --model_size base --MoMuK
```

After training process, you can generate the captions of all the molecules in the test dataset. Run these command:

```
python main_transformer_smiles2caption.py --mode test --model_size base --output_file out.txt
python main_transformer_smiles2caption.py --mode test --model_size base --output_file out.txt --MoMuK
```

A file `out.txt` will be generated. It contains all the SMILES strings, caption ground truths and captions generated by MoMu-enhanced MolT5, just like:
```
SMILES	ground truth	output
CCCCCCCCCCCCCCCCCCCCCC[C@H](C(=O)N[C@@H](CO[C@H]1[C@@H]([C@H]([C@H]([C@H](O1)CO)O)OS(=O)(=O)O)O)[C@@H](/C=C/CCCCCCCCCCCCC)O)O	The molecule is a galactosylceramide sulfate in which the sulfo group is located at position 3 and the ceramide N-acyl group is specified as (R)-2-hydroxylignoceroyl. It is a N-acyl-beta-D-galactosylsphingosine and a galactosylceramide sulfate. It derives from a (R)-2-hydroxylignoceric acid. It is a conjugate acid of a 1-(3-O-sulfo-beta-D-galactosyl)-N-[(2R)-2-hydroxylignoceroyl]sphingosine(1-).	The molecule is a galactosylceramide sulfate in which the ceramide N-acyl group is specified as (R)-2-hydroxybehenoyl. It is a galactosylceramide sulfate and a N-acyl-beta-D-galactosylsphingosine. It is a conjugate acid of a 1-(3-O-sulfo-beta-D-galactosyl)-N-[(2R)-2-hydroxybehenoyl]sphingosine(1-).
...
...
```


# Evaluation
To evaluate the molecule caption task, we just need the `out.txt` generated by MoMu-enhanced MolT5.
We provide an example `out.txt` in this repository. So you can evaluate the performance without running the finetune & generate captions commands.

## The BLEU & Rouge & Meteor metrics
Run this command to evaluate all NLG metrics:
```
python text_translation_metrics.py --input_file out.txt
```

## The Text2Mol metric
Before Evaluating the Text2Mol metric, download the `cid_to_smiles.pkl` file from https://uofi.box.com/v/MolT5-cid-to-smiles and `test_outputfinal_weights.320.pt` form https://uofi.box.com/s/es16alnhzfy1hpagf55fu48k49f8n29x. `cid_to_smiles.pkl` should be placed in the root path of the project and `test_outputfinal_weights.320.pt` should be placed in the `t2m_output/` folder. Then run this command:

```
python text_text2mol_metric.py --use_gt --input_file out.txt
```

# Citation

```
@article{su2022molecular,
  title={Natural Language-informed Understanding of Molecule Graphs},
  author={Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, Ji-Rong Wen},
  year={2022}
}
```